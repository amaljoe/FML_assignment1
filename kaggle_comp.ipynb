{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-24T12:20:31.755490Z",
     "start_time": "2024-08-24T12:20:31.263652Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.f2py.crackfortran import verbose\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T12:20:32.167149Z",
     "start_time": "2024-08-24T12:20:31.892390Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the data\n",
    "train = pd.read_csv('iitb-cs-725-1-2024/train.csv')\n",
    "test = pd.read_csv('iitb-cs-725-1-2024/test.csv')\n",
    "sample = pd.read_csv('iitb-cs-725-1-2024/sample.csv')"
   ],
   "id": "e22c4fd90e032e7a",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T06:34:13.690787Z",
     "start_time": "2024-08-23T06:34:13.677740Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Display the data\n",
    "train.head()"
   ],
   "id": "fb0600bd244c422f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   ID  feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
       "0   0   0.040776   2.247366   0.486094  -1.791148  -0.260466  -0.693494   \n",
       "1   1   2.885314  -1.228567  -0.023254   1.243954  -0.276285   0.791822   \n",
       "2   2   2.612578   0.978922   0.402049   0.628482   0.481488  -0.421969   \n",
       "3   3   0.333623   0.658054  -0.747531  -0.505702   0.359609   0.459562   \n",
       "4   4  -0.873343  -0.650602   0.336838   0.100677  -0.317427  -0.072732   \n",
       "\n",
       "   feature_6  feature_7  feature_8  ...  feature_55  feature_56  feature_57  \\\n",
       "0  -0.007988  -0.390353   0.342489  ...    0.116905    0.004497    0.045408   \n",
       "1  -0.390133   0.048296  -0.007136  ...    0.111936   -0.050685   -0.163769   \n",
       "2  -0.090971  -0.496587   0.314399  ...   -0.066646   -0.092024   -0.078880   \n",
       "3   0.278906   0.078858   0.330526  ...    0.121561    0.065324    0.114508   \n",
       "4  -0.187683   0.493616  -0.234106  ...    0.143080    0.051453    0.032550   \n",
       "\n",
       "   feature_58  feature_59  feature_60  feature_61  feature_62  feature_63  \\\n",
       "0   -0.160795    0.018927    0.038400   -0.224252   -0.095052   -0.060982   \n",
       "1   -0.068437    0.082455   -0.013019   -0.052774   -0.042545   -0.027698   \n",
       "2   -0.005543    0.104462    0.136459   -0.064776    0.074138    0.030638   \n",
       "3    0.007231    0.055410    0.016136   -0.049182   -0.059409   -0.144704   \n",
       "4   -0.088987    0.003385    0.097682   -0.020679   -0.115271   -0.014201   \n",
       "\n",
       "   score  \n",
       "0    0.0  \n",
       "1    3.0  \n",
       "2    0.0  \n",
       "3    3.0  \n",
       "4    3.0  \n",
       "\n",
       "[5 rows x 66 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_55</th>\n",
       "      <th>feature_56</th>\n",
       "      <th>feature_57</th>\n",
       "      <th>feature_58</th>\n",
       "      <th>feature_59</th>\n",
       "      <th>feature_60</th>\n",
       "      <th>feature_61</th>\n",
       "      <th>feature_62</th>\n",
       "      <th>feature_63</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.040776</td>\n",
       "      <td>2.247366</td>\n",
       "      <td>0.486094</td>\n",
       "      <td>-1.791148</td>\n",
       "      <td>-0.260466</td>\n",
       "      <td>-0.693494</td>\n",
       "      <td>-0.007988</td>\n",
       "      <td>-0.390353</td>\n",
       "      <td>0.342489</td>\n",
       "      <td>...</td>\n",
       "      <td>0.116905</td>\n",
       "      <td>0.004497</td>\n",
       "      <td>0.045408</td>\n",
       "      <td>-0.160795</td>\n",
       "      <td>0.018927</td>\n",
       "      <td>0.038400</td>\n",
       "      <td>-0.224252</td>\n",
       "      <td>-0.095052</td>\n",
       "      <td>-0.060982</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2.885314</td>\n",
       "      <td>-1.228567</td>\n",
       "      <td>-0.023254</td>\n",
       "      <td>1.243954</td>\n",
       "      <td>-0.276285</td>\n",
       "      <td>0.791822</td>\n",
       "      <td>-0.390133</td>\n",
       "      <td>0.048296</td>\n",
       "      <td>-0.007136</td>\n",
       "      <td>...</td>\n",
       "      <td>0.111936</td>\n",
       "      <td>-0.050685</td>\n",
       "      <td>-0.163769</td>\n",
       "      <td>-0.068437</td>\n",
       "      <td>0.082455</td>\n",
       "      <td>-0.013019</td>\n",
       "      <td>-0.052774</td>\n",
       "      <td>-0.042545</td>\n",
       "      <td>-0.027698</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2.612578</td>\n",
       "      <td>0.978922</td>\n",
       "      <td>0.402049</td>\n",
       "      <td>0.628482</td>\n",
       "      <td>0.481488</td>\n",
       "      <td>-0.421969</td>\n",
       "      <td>-0.090971</td>\n",
       "      <td>-0.496587</td>\n",
       "      <td>0.314399</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.066646</td>\n",
       "      <td>-0.092024</td>\n",
       "      <td>-0.078880</td>\n",
       "      <td>-0.005543</td>\n",
       "      <td>0.104462</td>\n",
       "      <td>0.136459</td>\n",
       "      <td>-0.064776</td>\n",
       "      <td>0.074138</td>\n",
       "      <td>0.030638</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.333623</td>\n",
       "      <td>0.658054</td>\n",
       "      <td>-0.747531</td>\n",
       "      <td>-0.505702</td>\n",
       "      <td>0.359609</td>\n",
       "      <td>0.459562</td>\n",
       "      <td>0.278906</td>\n",
       "      <td>0.078858</td>\n",
       "      <td>0.330526</td>\n",
       "      <td>...</td>\n",
       "      <td>0.121561</td>\n",
       "      <td>0.065324</td>\n",
       "      <td>0.114508</td>\n",
       "      <td>0.007231</td>\n",
       "      <td>0.055410</td>\n",
       "      <td>0.016136</td>\n",
       "      <td>-0.049182</td>\n",
       "      <td>-0.059409</td>\n",
       "      <td>-0.144704</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-0.873343</td>\n",
       "      <td>-0.650602</td>\n",
       "      <td>0.336838</td>\n",
       "      <td>0.100677</td>\n",
       "      <td>-0.317427</td>\n",
       "      <td>-0.072732</td>\n",
       "      <td>-0.187683</td>\n",
       "      <td>0.493616</td>\n",
       "      <td>-0.234106</td>\n",
       "      <td>...</td>\n",
       "      <td>0.143080</td>\n",
       "      <td>0.051453</td>\n",
       "      <td>0.032550</td>\n",
       "      <td>-0.088987</td>\n",
       "      <td>0.003385</td>\n",
       "      <td>0.097682</td>\n",
       "      <td>-0.020679</td>\n",
       "      <td>-0.115271</td>\n",
       "      <td>-0.014201</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 66 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 107
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T05:39:44.077631Z",
     "start_time": "2024-08-23T05:39:43.974027Z"
    }
   },
   "cell_type": "code",
   "source": "train.describe()",
   "id": "9fbeacd7f2c52355",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                 ID     feature_0     feature_1     feature_2     feature_3  \\\n",
       "count  34988.000000  34988.000000  34988.000000  34988.000000  34988.000000   \n",
       "mean   17493.500000     -0.025667      0.005326     -0.015741     -0.004780   \n",
       "std    10100.309946      3.030748      2.171696      0.909885      0.676247   \n",
       "min        0.000000    -15.945959     -6.853446     -3.450916     -3.371233   \n",
       "25%     8746.750000     -2.061522     -1.405245     -0.617246     -0.439032   \n",
       "50%    17493.500000      0.230517     -0.278906     -0.073073     -0.033572   \n",
       "75%    26240.250000      2.271520      1.090409      0.530538      0.414171   \n",
       "max    34987.000000      8.828334     20.064835      7.041929      4.111352   \n",
       "\n",
       "          feature_4     feature_5     feature_6     feature_7     feature_8  \\\n",
       "count  34988.000000  34988.000000  34988.000000  34988.000000  34988.000000   \n",
       "mean      -0.008944     -0.004670     -0.018182      0.004407     -0.006715   \n",
       "std        0.625618      0.529283      0.490776      0.446237      0.411561   \n",
       "min       -6.351802     -2.003163     -1.482526     -2.353964     -1.659474   \n",
       "25%       -0.372987     -0.368630     -0.356968     -0.241367     -0.279038   \n",
       "50%        0.030238     -0.033410     -0.055696      0.040935     -0.012411   \n",
       "75%        0.409971      0.330293      0.281676      0.280784      0.260926   \n",
       "max        1.978701      2.511990      6.510805      8.411146      2.536084   \n",
       "\n",
       "       ...    feature_55    feature_56    feature_57    feature_58  \\\n",
       "count  ...  34988.000000  34988.000000  34988.000000  34988.000000   \n",
       "mean   ...     -0.000510      0.000518      0.000264     -0.001224   \n",
       "std    ...      0.095402      0.093816      0.093434      0.091614   \n",
       "min    ...     -0.942247     -0.588330     -0.771596     -0.597644   \n",
       "25%    ...     -0.058841     -0.060398     -0.060151     -0.058916   \n",
       "50%    ...     -0.000265     -0.000856      0.000478     -0.000103   \n",
       "75%    ...      0.058520      0.059978      0.061082      0.057753   \n",
       "max    ...      0.975914      0.696887      0.532025      0.622193   \n",
       "\n",
       "         feature_59    feature_60    feature_61    feature_62    feature_63  \\\n",
       "count  34988.000000  34988.000000  34988.000000  34988.000000  34988.000000   \n",
       "mean       0.001429      0.001177      0.000770      0.000382      0.000216   \n",
       "std        0.090396      0.089828      0.088973      0.087636      0.087134   \n",
       "min       -0.495204     -0.429300     -0.509795     -0.415701     -0.629959   \n",
       "25%       -0.056803     -0.055921     -0.056793     -0.056750     -0.055648   \n",
       "50%        0.002216      0.001734      0.000313     -0.000057      0.000577   \n",
       "75%        0.059987      0.058986      0.057766      0.057200      0.056666   \n",
       "max        0.452759      0.549956      0.471785      0.565583      0.621282   \n",
       "\n",
       "              score  \n",
       "count  34988.000000  \n",
       "mean       3.144707  \n",
       "std        1.162853  \n",
       "min        0.000000  \n",
       "25%        3.000000  \n",
       "50%        4.000000  \n",
       "75%        4.000000  \n",
       "max        4.000000  \n",
       "\n",
       "[8 rows x 66 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_55</th>\n",
       "      <th>feature_56</th>\n",
       "      <th>feature_57</th>\n",
       "      <th>feature_58</th>\n",
       "      <th>feature_59</th>\n",
       "      <th>feature_60</th>\n",
       "      <th>feature_61</th>\n",
       "      <th>feature_62</th>\n",
       "      <th>feature_63</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>34988.000000</td>\n",
       "      <td>34988.000000</td>\n",
       "      <td>34988.000000</td>\n",
       "      <td>34988.000000</td>\n",
       "      <td>34988.000000</td>\n",
       "      <td>34988.000000</td>\n",
       "      <td>34988.000000</td>\n",
       "      <td>34988.000000</td>\n",
       "      <td>34988.000000</td>\n",
       "      <td>34988.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>34988.000000</td>\n",
       "      <td>34988.000000</td>\n",
       "      <td>34988.000000</td>\n",
       "      <td>34988.000000</td>\n",
       "      <td>34988.000000</td>\n",
       "      <td>34988.000000</td>\n",
       "      <td>34988.000000</td>\n",
       "      <td>34988.000000</td>\n",
       "      <td>34988.000000</td>\n",
       "      <td>34988.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>17493.500000</td>\n",
       "      <td>-0.025667</td>\n",
       "      <td>0.005326</td>\n",
       "      <td>-0.015741</td>\n",
       "      <td>-0.004780</td>\n",
       "      <td>-0.008944</td>\n",
       "      <td>-0.004670</td>\n",
       "      <td>-0.018182</td>\n",
       "      <td>0.004407</td>\n",
       "      <td>-0.006715</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000510</td>\n",
       "      <td>0.000518</td>\n",
       "      <td>0.000264</td>\n",
       "      <td>-0.001224</td>\n",
       "      <td>0.001429</td>\n",
       "      <td>0.001177</td>\n",
       "      <td>0.000770</td>\n",
       "      <td>0.000382</td>\n",
       "      <td>0.000216</td>\n",
       "      <td>3.144707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>10100.309946</td>\n",
       "      <td>3.030748</td>\n",
       "      <td>2.171696</td>\n",
       "      <td>0.909885</td>\n",
       "      <td>0.676247</td>\n",
       "      <td>0.625618</td>\n",
       "      <td>0.529283</td>\n",
       "      <td>0.490776</td>\n",
       "      <td>0.446237</td>\n",
       "      <td>0.411561</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095402</td>\n",
       "      <td>0.093816</td>\n",
       "      <td>0.093434</td>\n",
       "      <td>0.091614</td>\n",
       "      <td>0.090396</td>\n",
       "      <td>0.089828</td>\n",
       "      <td>0.088973</td>\n",
       "      <td>0.087636</td>\n",
       "      <td>0.087134</td>\n",
       "      <td>1.162853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-15.945959</td>\n",
       "      <td>-6.853446</td>\n",
       "      <td>-3.450916</td>\n",
       "      <td>-3.371233</td>\n",
       "      <td>-6.351802</td>\n",
       "      <td>-2.003163</td>\n",
       "      <td>-1.482526</td>\n",
       "      <td>-2.353964</td>\n",
       "      <td>-1.659474</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.942247</td>\n",
       "      <td>-0.588330</td>\n",
       "      <td>-0.771596</td>\n",
       "      <td>-0.597644</td>\n",
       "      <td>-0.495204</td>\n",
       "      <td>-0.429300</td>\n",
       "      <td>-0.509795</td>\n",
       "      <td>-0.415701</td>\n",
       "      <td>-0.629959</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8746.750000</td>\n",
       "      <td>-2.061522</td>\n",
       "      <td>-1.405245</td>\n",
       "      <td>-0.617246</td>\n",
       "      <td>-0.439032</td>\n",
       "      <td>-0.372987</td>\n",
       "      <td>-0.368630</td>\n",
       "      <td>-0.356968</td>\n",
       "      <td>-0.241367</td>\n",
       "      <td>-0.279038</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.058841</td>\n",
       "      <td>-0.060398</td>\n",
       "      <td>-0.060151</td>\n",
       "      <td>-0.058916</td>\n",
       "      <td>-0.056803</td>\n",
       "      <td>-0.055921</td>\n",
       "      <td>-0.056793</td>\n",
       "      <td>-0.056750</td>\n",
       "      <td>-0.055648</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>17493.500000</td>\n",
       "      <td>0.230517</td>\n",
       "      <td>-0.278906</td>\n",
       "      <td>-0.073073</td>\n",
       "      <td>-0.033572</td>\n",
       "      <td>0.030238</td>\n",
       "      <td>-0.033410</td>\n",
       "      <td>-0.055696</td>\n",
       "      <td>0.040935</td>\n",
       "      <td>-0.012411</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000265</td>\n",
       "      <td>-0.000856</td>\n",
       "      <td>0.000478</td>\n",
       "      <td>-0.000103</td>\n",
       "      <td>0.002216</td>\n",
       "      <td>0.001734</td>\n",
       "      <td>0.000313</td>\n",
       "      <td>-0.000057</td>\n",
       "      <td>0.000577</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>26240.250000</td>\n",
       "      <td>2.271520</td>\n",
       "      <td>1.090409</td>\n",
       "      <td>0.530538</td>\n",
       "      <td>0.414171</td>\n",
       "      <td>0.409971</td>\n",
       "      <td>0.330293</td>\n",
       "      <td>0.281676</td>\n",
       "      <td>0.280784</td>\n",
       "      <td>0.260926</td>\n",
       "      <td>...</td>\n",
       "      <td>0.058520</td>\n",
       "      <td>0.059978</td>\n",
       "      <td>0.061082</td>\n",
       "      <td>0.057753</td>\n",
       "      <td>0.059987</td>\n",
       "      <td>0.058986</td>\n",
       "      <td>0.057766</td>\n",
       "      <td>0.057200</td>\n",
       "      <td>0.056666</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>34987.000000</td>\n",
       "      <td>8.828334</td>\n",
       "      <td>20.064835</td>\n",
       "      <td>7.041929</td>\n",
       "      <td>4.111352</td>\n",
       "      <td>1.978701</td>\n",
       "      <td>2.511990</td>\n",
       "      <td>6.510805</td>\n",
       "      <td>8.411146</td>\n",
       "      <td>2.536084</td>\n",
       "      <td>...</td>\n",
       "      <td>0.975914</td>\n",
       "      <td>0.696887</td>\n",
       "      <td>0.532025</td>\n",
       "      <td>0.622193</td>\n",
       "      <td>0.452759</td>\n",
       "      <td>0.549956</td>\n",
       "      <td>0.471785</td>\n",
       "      <td>0.565583</td>\n",
       "      <td>0.621282</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 66 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T05:39:47.298608Z",
     "start_time": "2024-08-23T05:39:47.292385Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check for missing values\n",
    "train.isnull().sum()"
   ],
   "id": "ea89dda4c1f12f25",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID            0\n",
       "feature_0     0\n",
       "feature_1     0\n",
       "feature_2     0\n",
       "feature_3     0\n",
       "             ..\n",
       "feature_60    0\n",
       "feature_61    0\n",
       "feature_62    0\n",
       "feature_63    0\n",
       "score         0\n",
       "Length: 66, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T12:22:38.927257Z",
     "start_time": "2024-08-24T12:22:38.909677Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create datasets\n",
    "def create_datasets(data):\n",
    "    X = data.drop(['score', 'ID'], axis=1)\n",
    "    y = data['score']\n",
    "    X = np.array(X)\n",
    "    y = np.array(y).reshape(-1, 1)\n",
    "    return X, y\n",
    "\n",
    "# Shuffle the data\n",
    "train = train.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "n_train = int(0.9 * train.shape[0])\n",
    "X_train, y_train = create_datasets(train[:n_train])\n",
    "X_val, y_val = create_datasets(train[n_train:])\n",
    "\n",
    "X_train.shape, X_val.shape"
   ],
   "id": "51514d8dfadf7680",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((31489, 64), (3499, 64))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T06:34:52.156571Z",
     "start_time": "2024-08-23T06:34:52.153614Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# No. of features\n",
    "N = X_train.shape[1]\n",
    "\n",
    "# Weight vector (N x 1)\n",
    "W = np.random.randn(N, 1)"
   ],
   "id": "e762e814123f10dc",
   "outputs": [],
   "execution_count": 110
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T06:23:26.318252Z",
     "start_time": "2024-08-23T06:23:23.644557Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.Input(shape=(N,)),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, input_shape=(N,))\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10)"
   ],
   "id": "a5fcf1d82994fa15",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001B[1m  1/875\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m2:35\u001B[0m 178ms/step - loss: 10.9344"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amaljoe/Desktop/Workspace/IITB/FML/assignment1/venv/lib/python3.9/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m875/875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 320us/step - loss: 3.2019 - val_loss: 0.9124\n",
      "Epoch 2/10\n",
      "\u001B[1m875/875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 257us/step - loss: 0.9180 - val_loss: 0.8635\n",
      "Epoch 3/10\n",
      "\u001B[1m875/875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 283us/step - loss: 0.8648 - val_loss: 0.8327\n",
      "Epoch 4/10\n",
      "\u001B[1m875/875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 256us/step - loss: 0.8300 - val_loss: 0.8150\n",
      "Epoch 5/10\n",
      "\u001B[1m875/875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 262us/step - loss: 0.8102 - val_loss: 0.8148\n",
      "Epoch 6/10\n",
      "\u001B[1m875/875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 252us/step - loss: 0.8063 - val_loss: 0.7958\n",
      "Epoch 7/10\n",
      "\u001B[1m875/875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 271us/step - loss: 0.7995 - val_loss: 0.7955\n",
      "Epoch 8/10\n",
      "\u001B[1m875/875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 278us/step - loss: 0.7886 - val_loss: 0.7991\n",
      "Epoch 9/10\n",
      "\u001B[1m875/875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 283us/step - loss: 0.7764 - val_loss: 0.7943\n",
      "Epoch 10/10\n",
      "\u001B[1m875/875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 276us/step - loss: 0.7928 - val_loss: 0.8037\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x38de1ab50>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T13:27:54.079168Z",
     "start_time": "2024-08-24T13:27:54.072211Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "\n",
    "def predict(X, W):\n",
    "    return X @ W\n",
    "\n",
    "def mse(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred)**2)\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mse(y_true, y_pred))\n",
    "\n",
    "def compute_gradients(X, y, y_pred):\n",
    "    dW = -2 * X.T @ (y - y_pred) / X.shape[0]\n",
    "    return dW\n",
    "\n",
    "def fit(X, y, W, Xval, yval, lr=0.01, epochs=100, print_every=100, batch_size=32):\n",
    "    # Error list\n",
    "    errors = []\n",
    "    val_errors = []\n",
    "    best_val_error = float('inf')\n",
    "    best_W = None\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        # Mini-batch gradient descent\n",
    "        idx = random.sample(range(X.shape[0]), batch_size)\n",
    "        X_batch, y_batch = X[idx], y[idx]\n",
    "        y_pred = predict(X_batch, W)\n",
    "        y_pred_round = y_pred.round()\n",
    "\n",
    "        if i % print_every == 0 or i == epochs - 1:\n",
    "            train_loss = mse(y, predict(X, W))\n",
    "            y_val_pred = predict(Xval, W)\n",
    "            val_loss = mse(yval, y_val_pred)\n",
    "            Y_val_round = y_val_pred.round()\n",
    "            Y_val_rounded_loss = mse(yval, Y_val_round)\n",
    "            print(f'Epoch {i}, Loss: {train_loss}, Val Loss: {val_loss}, Rounded Val Loss: {Y_val_rounded_loss}')\n",
    "            errors.append(train_loss)\n",
    "            val_errors.append(val_loss)\n",
    "            if val_loss < best_val_error:\n",
    "                best_val_error = val_loss\n",
    "                best_W = W\n",
    "            if train_loss < 0.8 and val_loss < 0.87:\n",
    "                print(f'Early stopping at epoch {i}, Loss: {train_loss}, Val Loss: {val_loss}')\n",
    "                break\n",
    "        \n",
    "        dW = compute_gradients(X_batch, y_batch, y_pred_round)\n",
    "        W -= lr * dW\n",
    "        \n",
    "        \n",
    "    return best_W, errors, val_errors"
   ],
   "id": "c180fbdca030deee",
   "outputs": [],
   "execution_count": 107
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T12:20:47.842247Z",
     "start_time": "2024-08-24T12:20:47.807622Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Closed form solution\n",
    "\n",
    "from closedForm import LinearRegressionClosedForm\n",
    "\n",
    "model = LinearRegressionClosedForm()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_val)\n",
    "rmse(y_val, y_pred)"
   ],
   "id": "4c91e943fbd0d643",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.2682933059181574"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T12:20:46.293882Z",
     "start_time": "2024-08-24T12:20:46.290019Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_errors(errors, val_errors):\n",
    "    plt.plot(errors, label='Train')\n",
    "    plt.plot(val_errors, label='Validation')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "id": "b5a34a0664c55127",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T12:20:50.335821Z",
     "start_time": "2024-08-24T12:20:50.194971Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Gradient Descent\n",
    "W, errors, val_errors = fit(X_train, y_train, W, X_val, y_val, lr=0.0001, epochs=1000)\n",
    "\n",
    "y_pred = predict(X_val, W)\n",
    "\n",
    "plot_errors(errors, val_errors)\n",
    "\n",
    "rmse(y_val, y_pred)"
   ],
   "id": "a9b9a9aed346322b",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'W' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Gradient Descent\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m W, errors, val_errors \u001B[38;5;241m=\u001B[39m fit(X_train, y_train, \u001B[43mW\u001B[49m, X_val, y_val, lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.0001\u001B[39m, epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1000\u001B[39m)\n\u001B[1;32m      4\u001B[0m y_pred \u001B[38;5;241m=\u001B[39m predict(X_val, W)\n\u001B[1;32m      6\u001B[0m plot_errors(errors, val_errors)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'W' is not defined"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T13:29:04.003409Z",
     "start_time": "2024-08-24T13:29:03.965512Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Feature engineering\n",
    "\n",
    "# Mean and variance\n",
    "mu = np.mean(X_train)\n",
    "sigma = np.std(X_train)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def gaussian_basis(x, mu, sigma):\n",
    "    return np.exp(-0.5 * ((x - mu) / sigma)**2)\n",
    "\n",
    "def normalize(X, mu, sigma):\n",
    "    return (X - mu) / sigma\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Add polynomial features\n",
    "def transform_input(X):\n",
    "    X_new = X\n",
    "    for i in range(-1, 1):\n",
    "        j = i\n",
    "        X_new = np.hstack([X_new, gaussian_basis(X, mu + j, sigma)])\n",
    "    return X_new\n",
    "        \n",
    "\n",
    "X_train_poly = transform_input(X_train)\n",
    "X_val_poly = transform_input(X_val)\n",
    "print(X_train_poly.shape, X_val_poly.shape)\n",
    "\n",
    "W_poly = np.random.randn(X_train_poly.shape[1], 1)"
   ],
   "id": "486672be6ba5336c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31489, 192) (3499, 192)\n"
     ]
    }
   ],
   "execution_count": 110
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T12:21:19.252656Z",
     "start_time": "2024-08-24T12:21:19.249889Z"
    }
   },
   "cell_type": "code",
   "source": "W_poly = np.load('weights5.npy')",
   "id": "88eca1d4ebe01ada",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T13:29:44.111818Z",
     "start_time": "2024-08-24T13:29:33.656841Z"
    }
   },
   "cell_type": "code",
   "source": [
    "epochs = 300000\n",
    "\n",
    "W_train_poly, errors, val_errors = fit(X_train_poly, y_train, W_poly, X_val_poly, y_val, lr=0.0001, epochs=epochs, print_every=epochs/10, batch_size=32)\n",
    "\n",
    "plot_errors(errors, val_errors)\n",
    "\n",
    "y_pred = predict(X_val_poly, W_train_poly)\n",
    "y_pred_round = y_pred.round()\n",
    "mse(y_val, y_pred_round)"
   ],
   "id": "ed943654e030999e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.8939376687491218, Val Loss: 0.8698722453427313, Rounded Val Loss: 0.948556730494427\n",
      "Epoch 30000, Loss: 0.8929320053372688, Val Loss: 0.8692244692917621, Rounded Val Loss: 0.9465561589025436\n",
      "Epoch 60000, Loss: 0.8925326080432847, Val Loss: 0.8698841066325171, Rounded Val Loss: 0.9531294655615891\n",
      "Epoch 90000, Loss: 0.8925428295884198, Val Loss: 0.8681331197881361, Rounded Val Loss: 0.9476993426693341\n",
      "Epoch 120000, Loss: 0.8920606716102898, Val Loss: 0.8696597136390192, Rounded Val Loss: 0.9502715061446128\n",
      "Epoch 150000, Loss: 0.8915347174610333, Val Loss: 0.8682482954769237, Rounded Val Loss: 0.9474135467276364\n",
      "Epoch 180000, Loss: 0.8911671379047866, Val Loss: 0.8676751012748977, Rounded Val Loss: 0.9459845670191483\n",
      "Epoch 210000, Loss: 0.8908797950916196, Val Loss: 0.8671448961065406, Rounded Val Loss: 0.9476993426693341\n",
      "Epoch 240000, Loss: 0.8906352946268776, Val Loss: 0.8674741127599412, Rounded Val Loss: 0.9468419548442412\n",
      "Epoch 270000, Loss: 0.8903830279547885, Val Loss: 0.8674458014384347, Rounded Val Loss: 0.9488425264361247\n",
      "Epoch 299999, Loss: 0.8900494273532221, Val Loss: 0.867231611836957, Rounded Val Loss: 0.9442697913689626\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.9451271791940554"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 113
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T13:30:05.267630Z",
     "start_time": "2024-08-24T13:30:05.264523Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# export weights\n",
    "np.save('weights6.npy', W_train_poly)\n",
    "W_backup = W_train_poly"
   ],
   "id": "bc4407bfda51ec3f",
   "outputs": [],
   "execution_count": 114
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T13:03:31.403706Z",
     "start_time": "2024-08-24T13:03:31.278559Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_val_poly = transform_input(X_val)\n",
    "y_pred = predict(X_val_poly, W_poly)\n",
    "\n",
    "rmse(y_val, y_pred)"
   ],
   "id": "2aea3318c3e290d1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9318112443972046"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 98
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T13:04:08.769058Z",
     "start_time": "2024-08-24T13:04:08.650302Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get output for final test data\n",
    "X_final_test = np.array(test.drop('ID', axis=1))\n",
    "X_final_test_poly = transform_input(X_final_test)\n",
    "y_final_test = predict(X_final_test_poly, W_poly)\n",
    "y_final_test = y_final_test.round()\n",
    "\n",
    "output = pd.DataFrame({'ID': test['ID'], 'score': y_final_test.flatten()})\n",
    "\n",
    "# output['score'] = output['score'].clip(0, 5)\n",
    "\n",
    "# count score < 0\n",
    "below0 = output[output['score'] < 0].shape[0]\n",
    "\n",
    "# count score > 5\n",
    "above5 = output[output['score'] > 5].shape[0]\n",
    "\n",
    "print(output.shape[0], below0, above5)\n",
    "\n",
    "output.to_csv('output.csv', index=False)"
   ],
   "id": "6a41c6871d40ef33",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14996 0 3\n"
     ]
    }
   ],
   "execution_count": 100
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T06:19:37.799872Z",
     "start_time": "2024-08-23T06:19:27.643080Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def gaussian_basis(x):\n",
    "    # Mean and variance\n",
    "    mu = np.mean(x)\n",
    "    sigma = np.std(x)\n",
    "    return np.exp(-0.5 * ((x - mu) / sigma)**2)\n",
    "\n",
    "def normalize(X):\n",
    "    return (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "\n",
    "# Add polynomial features\n",
    "def transform_input(X, basis):\n",
    "    transormed_X = np.hstack([basis[i](X) for i in range(len(basis))])\n",
    "    transormed_X = np.hstack([X, transormed_X])\n",
    "    return transormed_X\n",
    "\n",
    "basis_funcs = [\n",
    "    [normalize],\n",
    "    [gaussian_basis],\n",
    "    [sigmoid],\n",
    "    [sigmoid, gaussian_basis, normalize],\n",
    "]\n",
    "\n",
    "for basis in basis_funcs:\n",
    "    transformed_X_train = transform_input(X_train, basis)\n",
    "    transformed_X_val = transform_input(X_val, basis)\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.Input(shape=(transformed_X_train.shape[1],)),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    \n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=opt, loss='mse')\n",
    "    model.fit(transformed_X_train, y_train, validation_data=(transformed_X_val, y_val), epochs=10, verbose=0)\n",
    "    train_loss, val_loss = model.evaluate(transformed_X_train, y_train), model.evaluate(transformed_X_val, y_val)\n",
    "    print(basis, train_loss, val_loss)"
   ],
   "id": "f1d0246721ba13c7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m875/875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 187us/step - loss: 0.8705\n",
      "\u001B[1m110/110\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 206us/step - loss: 0.8961\n",
      "[<function normalize at 0x38ef3daf0>] 0.8870629668235779 0.8694994449615479\n",
      "\u001B[1m875/875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 188us/step - loss: 0.8607\n",
      "\u001B[1m110/110\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 211us/step - loss: 0.8802\n",
      "[<function gaussian_basis at 0x3b3d63550>] 0.8777105212211609 0.8520484566688538\n",
      "\u001B[1m875/875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 189us/step - loss: 0.8657\n",
      "\u001B[1m110/110\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 221us/step - loss: 0.8883\n",
      "[<function sigmoid at 0x38375dee0>] 0.882866621017456 0.8620593547821045\n",
      "\u001B[1m875/875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 189us/step - loss: 0.8679\n",
      "\u001B[1m110/110\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 210us/step - loss: 0.8938\n",
      "[<function sigmoid at 0x38375dee0>, <function gaussian_basis at 0x3b3d63550>, <function normalize at 0x38ef3daf0>] 0.8849837779998779 0.8647343516349792\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T05:50:12.135187Z",
     "start_time": "2024-08-23T05:50:09.230259Z"
    }
   },
   "cell_type": "code",
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=opt, loss='mse')\n",
    "model.fit(transformed_X_train, y_train, validation_data=(transformed_X_val, y_val), epochs=10)"
   ],
   "id": "2b47c33d566cd0ef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001B[1m875/875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 349us/step - loss: 1.3638 - val_loss: 1.0372\n",
      "Epoch 2/10\n",
      "\u001B[1m875/875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 296us/step - loss: 0.9448 - val_loss: 0.9532\n",
      "Epoch 3/10\n",
      "\u001B[1m875/875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 296us/step - loss: 0.9095 - val_loss: 0.9712\n",
      "Epoch 4/10\n",
      "\u001B[1m875/875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 291us/step - loss: 0.8788 - val_loss: 0.9324\n",
      "Epoch 5/10\n",
      "\u001B[1m875/875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 290us/step - loss: 0.8806 - val_loss: 0.8915\n",
      "Epoch 6/10\n",
      "\u001B[1m875/875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 289us/step - loss: 0.8550 - val_loss: 0.9437\n",
      "Epoch 7/10\n",
      "\u001B[1m875/875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 295us/step - loss: 0.8317 - val_loss: 0.8486\n",
      "Epoch 8/10\n",
      "\u001B[1m875/875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 292us/step - loss: 0.8230 - val_loss: 0.8512\n",
      "Epoch 9/10\n",
      "\u001B[1m875/875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 294us/step - loss: 0.8435 - val_loss: 0.8431\n",
      "Epoch 10/10\n",
      "\u001B[1m875/875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 301us/step - loss: 0.8142 - val_loss: 0.8732\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x381fb0f70>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T07:12:49.563119Z",
     "start_time": "2024-08-23T07:12:49.532866Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Add polynomial features\n",
    "def transform_input(X, basis):\n",
    "    transormed_X = np.hstack([basis[i](X) for i in range(len(basis))])\n",
    "    transormed_X = np.hstack([X, transormed_X])\n",
    "    return transormed_X\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def gaussian_basis(x):\n",
    "    # Mean and variance\n",
    "    mu = np.mean(x)\n",
    "    sigma = np.std(x)\n",
    "    return np.exp(-0.5 * ((x - mu) / sigma)**2)\n",
    "\n",
    "basis = [gaussian_basis]\n",
    "\n",
    "transformed_X_train = transform_input(X_train, basis)\n",
    "transformed_X_val = transform_input(X_val, basis)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.Input(shape=(transformed_X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.summary()"
   ],
   "id": "b0ae2f862bc0269d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"sequential_111\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_111\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_134 (\u001B[38;5;33mDense\u001B[0m)               │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m1\u001B[0m)              │           \u001B[38;5;34m129\u001B[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_134 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m129\u001B[0m (516.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> (516.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m129\u001B[0m (516.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> (516.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 214
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T07:13:32.103994Z",
     "start_time": "2024-08-23T07:13:05.310034Z"
    }
   },
   "cell_type": "code",
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=opt, loss='mse')\n",
    "model.fit(transformed_X_train, y_train, validation_data=(transformed_X_val, y_val), epochs=100)"
   ],
   "id": "72800032e3ad2466",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 344us/step - loss: 3.3207 - val_loss: 1.4445\n",
      "Epoch 2/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 256us/step - loss: 1.3784 - val_loss: 1.3350\n",
      "Epoch 3/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 253us/step - loss: 1.2782 - val_loss: 1.2485\n",
      "Epoch 4/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 253us/step - loss: 1.2242 - val_loss: 1.1806\n",
      "Epoch 5/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 248us/step - loss: 1.1312 - val_loss: 1.1279\n",
      "Epoch 6/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 253us/step - loss: 1.0916 - val_loss: 1.0818\n",
      "Epoch 7/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 248us/step - loss: 1.0511 - val_loss: 1.0465\n",
      "Epoch 8/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 252us/step - loss: 1.0085 - val_loss: 1.0163\n",
      "Epoch 9/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 254us/step - loss: 0.9937 - val_loss: 0.9920\n",
      "Epoch 10/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 260us/step - loss: 0.9806 - val_loss: 0.9714\n",
      "Epoch 11/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 261us/step - loss: 0.9652 - val_loss: 0.9547\n",
      "Epoch 12/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 259us/step - loss: 0.9458 - val_loss: 0.9405\n",
      "Epoch 13/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 260us/step - loss: 0.9507 - val_loss: 0.9318\n",
      "Epoch 14/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 268us/step - loss: 0.9287 - val_loss: 0.9219\n",
      "Epoch 15/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 261us/step - loss: 0.9134 - val_loss: 0.9119\n",
      "Epoch 16/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 267us/step - loss: 0.9074 - val_loss: 0.9056\n",
      "Epoch 17/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 261us/step - loss: 0.9017 - val_loss: 0.8995\n",
      "Epoch 18/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 260us/step - loss: 0.9041 - val_loss: 0.8954\n",
      "Epoch 19/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 260us/step - loss: 0.9011 - val_loss: 0.8911\n",
      "Epoch 20/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 264us/step - loss: 0.8922 - val_loss: 0.8877\n",
      "Epoch 21/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 264us/step - loss: 0.8963 - val_loss: 0.8861\n",
      "Epoch 22/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 272us/step - loss: 0.8979 - val_loss: 0.8822\n",
      "Epoch 23/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 264us/step - loss: 0.8853 - val_loss: 0.8811\n",
      "Epoch 24/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 262us/step - loss: 0.8810 - val_loss: 0.8791\n",
      "Epoch 25/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 258us/step - loss: 0.8789 - val_loss: 0.8770\n",
      "Epoch 26/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 260us/step - loss: 0.9060 - val_loss: 0.8762\n",
      "Epoch 27/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 260us/step - loss: 0.8769 - val_loss: 0.8758\n",
      "Epoch 28/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 259us/step - loss: 0.8884 - val_loss: 0.8745\n",
      "Epoch 29/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 271us/step - loss: 0.8828 - val_loss: 0.8731\n",
      "Epoch 30/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 260us/step - loss: 0.8784 - val_loss: 0.8726\n",
      "Epoch 31/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 262us/step - loss: 0.8863 - val_loss: 0.8718\n",
      "Epoch 32/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 259us/step - loss: 0.8894 - val_loss: 0.8721\n",
      "Epoch 33/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 261us/step - loss: 0.8856 - val_loss: 0.8709\n",
      "Epoch 34/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 259us/step - loss: 0.8702 - val_loss: 0.8702\n",
      "Epoch 35/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 263us/step - loss: 0.8668 - val_loss: 0.8705\n",
      "Epoch 36/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 261us/step - loss: 0.8770 - val_loss: 0.8696\n",
      "Epoch 37/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 263us/step - loss: 0.8893 - val_loss: 0.8697\n",
      "Epoch 38/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 259us/step - loss: 0.8903 - val_loss: 0.8696\n",
      "Epoch 39/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 261us/step - loss: 0.8714 - val_loss: 0.8701\n",
      "Epoch 40/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 258us/step - loss: 0.8941 - val_loss: 0.8689\n",
      "Epoch 41/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 260us/step - loss: 0.8711 - val_loss: 0.8685\n",
      "Epoch 42/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 264us/step - loss: 0.8936 - val_loss: 0.8684\n",
      "Epoch 43/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 263us/step - loss: 0.8632 - val_loss: 0.8685\n",
      "Epoch 44/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 259us/step - loss: 0.8699 - val_loss: 0.8682\n",
      "Epoch 45/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 259us/step - loss: 0.8823 - val_loss: 0.8687\n",
      "Epoch 46/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 256us/step - loss: 0.8813 - val_loss: 0.8691\n",
      "Epoch 47/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 265us/step - loss: 0.8703 - val_loss: 0.8682\n",
      "Epoch 48/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 262us/step - loss: 0.8692 - val_loss: 0.8677\n",
      "Epoch 49/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 260us/step - loss: 0.8730 - val_loss: 0.8678\n",
      "Epoch 50/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 262us/step - loss: 0.8783 - val_loss: 0.8677\n",
      "Epoch 51/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 261us/step - loss: 0.8776 - val_loss: 0.8691\n",
      "Epoch 52/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 264us/step - loss: 0.8719 - val_loss: 0.8681\n",
      "Epoch 53/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 260us/step - loss: 0.8642 - val_loss: 0.8678\n",
      "Epoch 54/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 265us/step - loss: 0.8787 - val_loss: 0.8679\n",
      "Epoch 55/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 261us/step - loss: 0.8862 - val_loss: 0.8676\n",
      "Epoch 56/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 262us/step - loss: 0.8719 - val_loss: 0.8674\n",
      "Epoch 57/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 256us/step - loss: 0.8867 - val_loss: 0.8690\n",
      "Epoch 58/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 265us/step - loss: 0.8762 - val_loss: 0.8673\n",
      "Epoch 59/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 258us/step - loss: 0.8781 - val_loss: 0.8677\n",
      "Epoch 60/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 265us/step - loss: 0.8645 - val_loss: 0.8710\n",
      "Epoch 61/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 260us/step - loss: 0.8807 - val_loss: 0.8680\n",
      "Epoch 62/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 263us/step - loss: 0.8786 - val_loss: 0.8674\n",
      "Epoch 63/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 261us/step - loss: 0.8830 - val_loss: 0.8675\n",
      "Epoch 64/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 259us/step - loss: 0.8762 - val_loss: 0.8675\n",
      "Epoch 65/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 258us/step - loss: 0.8861 - val_loss: 0.8674\n",
      "Epoch 66/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 265us/step - loss: 0.8792 - val_loss: 0.8670\n",
      "Epoch 67/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 260us/step - loss: 0.8819 - val_loss: 0.8677\n",
      "Epoch 68/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 262us/step - loss: 0.8704 - val_loss: 0.8668\n",
      "Epoch 69/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 265us/step - loss: 0.8904 - val_loss: 0.8671\n",
      "Epoch 70/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 261us/step - loss: 0.8776 - val_loss: 0.8676\n",
      "Epoch 71/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 262us/step - loss: 0.8804 - val_loss: 0.8676\n",
      "Epoch 72/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 261us/step - loss: 0.8665 - val_loss: 0.8667\n",
      "Epoch 73/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 262us/step - loss: 0.8711 - val_loss: 0.8668\n",
      "Epoch 74/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 258us/step - loss: 0.8688 - val_loss: 0.8667\n",
      "Epoch 75/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 261us/step - loss: 0.8703 - val_loss: 0.8689\n",
      "Epoch 76/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 256us/step - loss: 0.8811 - val_loss: 0.8668\n",
      "Epoch 77/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 266us/step - loss: 0.8867 - val_loss: 0.8671\n",
      "Epoch 78/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 268us/step - loss: 0.8764 - val_loss: 0.8672\n",
      "Epoch 79/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 265us/step - loss: 0.8724 - val_loss: 0.8683\n",
      "Epoch 80/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 271us/step - loss: 0.8842 - val_loss: 0.8680\n",
      "Epoch 81/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 270us/step - loss: 0.8747 - val_loss: 0.8670\n",
      "Epoch 82/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 265us/step - loss: 0.8720 - val_loss: 0.8665\n",
      "Epoch 83/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 267us/step - loss: 0.8739 - val_loss: 0.8667\n",
      "Epoch 84/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 271us/step - loss: 0.8828 - val_loss: 0.8671\n",
      "Epoch 85/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 261us/step - loss: 0.8790 - val_loss: 0.8669\n",
      "Epoch 86/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 258us/step - loss: 0.8601 - val_loss: 0.8685\n",
      "Epoch 87/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 260us/step - loss: 0.8786 - val_loss: 0.8683\n",
      "Epoch 88/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 259us/step - loss: 0.8759 - val_loss: 0.8667\n",
      "Epoch 89/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 259us/step - loss: 0.8769 - val_loss: 0.8667\n",
      "Epoch 90/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 259us/step - loss: 0.8788 - val_loss: 0.8666\n",
      "Epoch 91/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 257us/step - loss: 0.8724 - val_loss: 0.8667\n",
      "Epoch 92/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 264us/step - loss: 0.8815 - val_loss: 0.8667\n",
      "Epoch 93/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 258us/step - loss: 0.8773 - val_loss: 0.8664\n",
      "Epoch 94/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 264us/step - loss: 0.8775 - val_loss: 0.8664\n",
      "Epoch 95/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 260us/step - loss: 0.8723 - val_loss: 0.8664\n",
      "Epoch 96/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 272us/step - loss: 0.8813 - val_loss: 0.8665\n",
      "Epoch 97/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 268us/step - loss: 0.8668 - val_loss: 0.8684\n",
      "Epoch 98/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 270us/step - loss: 0.8848 - val_loss: 0.8666\n",
      "Epoch 99/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 271us/step - loss: 0.8650 - val_loss: 0.8664\n",
      "Epoch 100/100\n",
      "\u001B[1m985/985\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 267us/step - loss: 0.8644 - val_loss: 0.8665\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x3b21817c0>"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 215
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T07:13:35.042685Z",
     "start_time": "2024-08-23T07:13:34.974191Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# RMSE\n",
    "rmse = tf.keras.metrics.RootMeanSquaredError()\n",
    "rmse.update_state(y_val, model.predict(transformed_X_val))\n",
    "rmse.result().numpy()"
   ],
   "id": "d6195f3c11d05e9c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m110/110\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 248us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.93085057"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 216
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "0.92 current best rmse on val set submitted\n",
    "0.87 ideally should be the best rmse on val set"
   ],
   "id": "17cadff8693a8a59"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8ec6cb201d3ede94"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
